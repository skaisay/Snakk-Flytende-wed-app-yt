#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ—Ä–≤–µ–∂—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Wikipedia –∏ –¥—Ä—É–≥–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
–°–æ–∑–¥–∞–µ—Ç JSON —Ñ–∞–π–ª—ã –¥–ª—è GitHub Pages –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Å–µ—Ä–≤–µ—Ä–∞—Ö
"""

import requests
import json
import time
from urllib.parse import quote
import trafilatura
import re
from datetime import datetime

class WikipediaScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Norwegian-Learning-App/1.0 (Educational purpose)'
        })
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.sources = {
            'wikipedia_no': 'https://no.wikipedia.org/w/api.php',
            'wiktionary_no': 'https://no.wiktionary.org/w/api.php',
            'conceptnet': 'https://api.conceptnet.io/query',
            'freqwords': 'https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/no/no_50k.txt'
        }
        
        self.data = []
        self.processed_words = set()
        
    def scrape_wikipedia_categories(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π Wikipedia"""
        print("üîç –ó–∞–≥—Ä—É–∂–∞—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –Ω–æ—Ä–≤–µ–∂—Å–∫–æ–π Wikipedia...")
        
        categories = [
            'Norske_ord',
            'Norsk_spr√•k', 
            'Norsk_grammatikk',
            'Norske_verb',
            'Norske_substantiv',
            'Norske_adjektiv',
            'Dagligdagse_uttrykk',
            'Norske_idiomer'
        ]
        
        for category in categories:
            try:
                print(f"  üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category}")
                
                params = {
                    'action': 'query',
                    'format': 'json',
                    'list': 'categorymembers',
                    'cmtitle': f'Category:{category}',
                    'cmlimit': '500'
                }
                
                response = self.session.get(self.sources['wikipedia_no'], params=params)
                data = response.json()
                
                if 'query' in data and 'categorymembers' in data['query']:
                    members = data['query']['categorymembers']
                    print(f"    üìÑ –ù–∞–π–¥–µ–Ω–æ {len(members)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    
                    for member in members[:100]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –Ω–∞—á–∞–ª–∞
                        self.process_wikipedia_page(member['title'])
                        time.sleep(0.1)  # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ API
                        
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}: {e}")
                
    def process_wikipedia_page(self, title):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Wikipedia"""
        if title in self.processed_words:
            return
            
        try:
            params = {
                'action': 'query',
                'format': 'json',
                'titles': title,
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'exsectionformat': 'plain'
            }
            
            response = self.session.get(self.sources['wikipedia_no'], params=params)
            data = response.json()
            
            if 'query' in data and 'pages' in data['query']:
                pages = data['query']['pages']
                for page_id, page_data in pages.items():
                    if page_id != '-1' and 'extract' in page_data:
                        extract = page_data['extract']
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥—ã
                        translations = self.extract_translations(extract, title)
                        for translation in translations:
                            if translation not in self.data:
                                self.data.append(translation)
                                
            self.processed_words.add(title)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {title}: {e}")
    
    def scrape_wiktionary(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è"""
        print("üìö –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ—Ä–≤–µ–∂—Å–∫–æ–≥–æ –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è...")
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ—Ä–≤–µ–∂—Å–∫–∏—Ö —Å–ª–æ–≤
        for _ in range(10):  # 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ 500 —Å—Ç—Ä–∞–Ω–∏—Ü = 5000 —Å–ª–æ–≤
            try:
                params = {
                    'action': 'query',
                    'format': 'json',
                    'list': 'random',
                    'rnnamespace': '0',
                    'rnlimit': '500'
                }
                
                response = self.session.get(self.sources['wiktionary_no'], params=params)
                data = response.json()
                
                if 'query' in data and 'random' in data['query']:
                    pages = data['query']['random']
                    print(f"  üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(pages)} —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
                    
                    for page in pages:
                        self.process_wiktionary_page(page['title'])
                        time.sleep(0.05)
                        
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è: {e}")
                
    def process_wiktionary_page(self, title):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è"""
        if title in self.processed_words:
            return
            
        try:
            params = {
                'action': 'query',
                'format': 'json',
                'titles': title,
                'prop': 'wikitext'
            }
            
            response = self.session.get(self.sources['wiktionary_no'], params=params)
            data = response.json()
            
            if 'query' in data and 'pages' in data['query']:
                pages = data['query']['pages']
                for page_id, page_data in pages.items():
                    if page_id != '-1' and 'revisions' in page_data:
                        wikitext = page_data['revisions'][0]['*']
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∏–∑ –≤–∏–∫–∏-—Ä–∞–∑–º–µ—Ç–∫–∏
                        translations = self.parse_wiktionary_text(wikitext, title)
                        for translation in translations:
                            if translation not in self.data:
                                self.data.append(translation)
                                
            self.processed_words.add(title)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è –¥–ª—è {title}: {e}")
    
    def scrape_frequency_words(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤"""
        print("üìä –ó–∞–≥—Ä—É–∂–∞—é —á–∞—Å—Ç–æ—Ç–Ω—ã–µ –Ω–æ—Ä–≤–µ–∂—Å–∫–∏–µ —Å–ª–æ–≤–∞...")
        
        try:
            response = self.session.get(self.sources['freqwords'])
            text = response.text
            
            lines = text.strip().split('\n')
            print(f"  üìÑ –ù–∞–π–¥–µ–Ω–æ {len(lines)} —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤")
            
            for i, line in enumerate(lines[:10000]):  # –ë–µ—Ä–µ–º —Ç–æ–ø 10k —Å–ª–æ–≤
                parts = line.split()
                if len(parts) >= 2:
                    word = parts[0].lower()
                    frequency = int(parts[1]) if parts[1].isdigit() else i + 1
                    
                    if word not in self.processed_words:
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ConceptNet
                        translation = self.get_conceptnet_translation(word)
                        
                        if translation:
                            self.data.append({
                                'no': word,
                                'ru': translation,
                                'frequency': frequency,
                                'source': 'frequency_list',
                                'category': 'common'
                            })
                            
                        self.processed_words.add(word)
                        
                        if i % 100 == 0:
                            print(f"    ‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i} —Å–ª–æ–≤...")
                            time.sleep(0.1)
                            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤: {e}")
    
    def get_conceptnet_translation(self, word):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ ConceptNet API"""
        try:
            url = f"{self.sources['conceptnet']}/c/no/{quote(word)}"
            response = self.session.get(url, timeout=5)
            data = response.json()
            
            if 'edges' in data:
                for edge in data['edges'][:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–≤—è–∑–µ–π
                    if edge['rel']['label'] == 'Synonym' or edge['rel']['label'] == 'TranslationOf':
                        if edge['end']['language'] == 'ru':
                            return edge['end']['label']
                        elif edge['start']['language'] == 'ru':
                            return edge['start']['label']
                            
        except Exception:
            pass
        return None
    
    def scrape_news_sites(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        print("üì∞ –ó–∞–≥—Ä—É–∂–∞—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        
        news_sites = [
            'https://www.nrk.no',
            'https://www.aftenposten.no',
            'https://www.dagbladet.no'
        ]
        
        for site in news_sites:
            try:
                print(f"  üåê –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {site}...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = self.session.get(site, timeout=10)
                text = trafilatura.extract(response.text)
                
                if text:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
                    expressions = self.extract_modern_expressions(text)
                    for expr in expressions:
                        if expr not in self.data:
                            self.data.append(expr)
                            
                time.sleep(1)  # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ —Å–∞–π—Ç—É
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {site}: {e}")
    
    def extract_translations(self, text, original_word):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        translations = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
        patterns = [
            r'—Ä—É—Å—Å–∫–∏–π[:\s]+([^.]+)',
            r'–ø–æ-—Ä—É—Å—Å–∫–∏[:\s]+([^.]+)',
            r'–æ–∑–Ω–∞—á–∞–µ—Ç[:\s]+([^.]+)',
            r'–ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫[:\s]+([^.]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                clean_translation = match.strip().strip('"\'')
                if clean_translation and len(clean_translation) < 100:
                    translations.append({
                        'no': original_word.lower(),
                        'ru': clean_translation,
                        'source': 'wikipedia',
                        'category': self.detect_category(text)
                    })
                    
        return translations
    
    def parse_wiktionary_text(self, wikitext, word):
        """–ü–∞—Ä—Å–∏—Ç –≤–∏–∫–∏-—Ç–µ–∫—Å—Ç –í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—è"""
        translations = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Å –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏
        russian_section = re.search(r'===?\s*—Ä—É—Å—Å–∫–∏–π\s*===?(.*?)(?===|\Z)', wikitext, re.IGNORECASE | re.DOTALL)
        if russian_section:
            section_text = russian_section.group(1)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã
            translation_patterns = [
                r'\[\[([^|\]]+)(?:\|[^\]]+)?\]\]',
                r"'''([^']+)'''",
                r'"([^"]+)"'
            ]
            
            for pattern in translation_patterns:
                matches = re.findall(pattern, section_text)
                for match in matches:
                    if match and len(match) < 50:
                        translations.append({
                            'no': word.lower(),
                            'ru': match.strip(),
                            'source': 'wiktionary',
                            'category': self.detect_category_from_wikitext(wikitext)
                        })
                        
        return translations
    
    def extract_modern_expressions(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        expressions = []
        
        # –ò—â–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ—Ä–º–∏–Ω—ã
        sentences = text.split('.')
        for sentence in sentences[:50]:  # –ü–µ—Ä–≤—ã–µ 50 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if len(sentence) > 20 and len(sentence) < 100:
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
                if any(keyword in sentence.lower() for keyword in ['ny', 'moderne', 'digital', 'teknologi']):
                    expressions.append({
                        'no': sentence.strip(),
                        'ru': '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ (—Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞)',
                        'source': 'news',
                        'category': 'modern'
                    })
                    
        return expressions[:20]  # –ú–∞–∫—Å–∏–º—É–º 20 –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞
    
    def detect_category(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–ª–æ–≤–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['–≥–ª–∞–≥–æ–ª', 'verb']):
            return 'verb'
        elif any(word in text_lower for word in ['—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ', 'noun', 'substantiv']):
            return 'noun'
        elif any(word in text_lower for word in ['–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ', 'adjective', 'adjektiv']):
            return 'adjective'
        elif any(word in text_lower for word in ['–Ω–∞—Ä–µ—á–∏–µ', 'adverb']):
            return 'adverb'
        else:
            return 'other'
    
    def detect_category_from_wikitext(self, wikitext):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –≤–∏–∫–∏-—Ä–∞–∑–º–µ—Ç–∫–∏"""
        if '{{verb' in wikitext.lower():
            return 'verb'
        elif '{{noun' in wikitext.lower() or '{{substantiv' in wikitext.lower():
            return 'noun'
        elif '{{adj' in wikitext.lower():
            return 'adjective'
        elif '{{adv' in wikitext.lower():
            return 'adverb'
        else:
            return 'other'
    
    def save_data(self, filename='norwegian_mega_data.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON"""
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_data = []
        seen = set()
        
        for item in self.data:
            key = f"{item['no']}-{item['ru']}"
            if key not in seen:
                seen.add(key)
                unique_data.append(item)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        output = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'total_records': len(unique_data),
                'sources': list(self.sources.keys()),
                'version': '1.0'
            },
            'data': unique_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        print(f"üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(unique_data):,}")
        
        return unique_data
    
    def run_full_scraping(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        print("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –Ω–æ—Ä–≤–µ–∂—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        print(f"üïê –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%H:%M:%S')}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.scrape_frequency_words()
        self.scrape_wiktionary()
        self.scrape_wikipedia_categories()
        self.scrape_news_sites()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        data = self.save_data()
        
        print("üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        
        sources_stats = {}
        for item in data:
            source = item.get('source', 'unknown')
            sources_stats[source] = sources_stats.get(source, 0) + 1
            
        for source, count in sources_stats.items():
            print(f"  {source}: {count:,} –∑–∞–ø–∏—Å–µ–π")
        
        return data

if __name__ == "__main__":
    scraper = WikipediaScraper()
    scraper.run_full_scraping()